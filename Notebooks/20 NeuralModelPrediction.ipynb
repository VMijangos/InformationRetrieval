{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836002b6",
   "metadata": {},
   "source": [
    "# Modelo neuronal para predicción de similitud\n",
    "\n",
    "Dentro de los modelos neuronales, una forma que podemos adoptar es que, al tener ya representaciones vectoriales tanto de los términos como de los documentos, usemos estos vectores como entradas a la red neuronal, de tal forma que ésta prediga la similitud entre el término y el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a44e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import brown\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c66be",
   "metadata": {},
   "source": [
    "### Preparación del dataset\n",
    "\n",
    "El data set con el que trabajaremos será supervisado; tendremos:\n",
    "\n",
    "$$\\mattcal{S} = \\{(x_{i,j}, y_{i,j}) : x_{i,j} \\in \\mathbb{R}^d, y_{i,j}\\in [0,1]\\}$$\n",
    "\n",
    "donde $x_{i,j}$ será un vector que contenga información del documento $j$ y del término $i$. Este vector lo crearemos al concatenar los vectores de ambos elementos. Los vectores los obtendremos con TFIDF.\n",
    "\n",
    "Por su parte $y_{i,j}$ es el valor de similitud que conocemos para estos datos. Como en los ejemplos supervisados, aquí debemos tener una supervisión que nos diga que valores esperamos de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4635ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4077)\n"
     ]
    }
   ],
   "source": [
    "#Colección de trabajo\n",
    "ids = brown.fileids(categories=['romance'])[:5] + brown.fileids(categories=['religion'])[:5]\n",
    "collection = [' '.join(brown.words(d)) for d in ids]\n",
    "\n",
    "#Vectorización con TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(collection)\n",
    "\n",
    "#Número de documentos y términos\n",
    "n_docs, n_terms = X.shape\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d36a64",
   "metadata": {},
   "source": [
    "Ahora obtenemos los términos y los documentos del modelo de TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e73a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Términos\n",
    "terms = vectorizer.vocabulary_\n",
    "#Documentos\n",
    "docs = {d:j for j,d in enumerate(ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff0adc",
   "metadata": {},
   "source": [
    "Para crear el dataset de entrenamiento, concatenamos los vectores que representan a los documentos y a los términos. En este caso, nuestra supervisión será ingenua, pues por simplicidad tomamos una predicción de 1 si el término está contenido en el documento y 0 si no. Una mejor supervisión podría llevar a mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed2044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectores de entrada\n",
    "x = []\n",
    "#Supervisión\n",
    "y = []\n",
    "\n",
    "for d in ids:\n",
    "    #Obtiene términos\n",
    "    words = [word.lower() for word in brown.words(d) if word.lower() in terms.keys()]\n",
    "    #Obtiene frecuencia de términos\n",
    "    word_frec = Counter(words)\n",
    "    for t in terms.keys():\n",
    "        #Obtiene vectores de término y documento en formato torch\n",
    "        t_vec, d_vec = torch.Tensor(X.T[terms[t]].todense()), torch.Tensor(X[docs[d]].todense())\n",
    "        #Concatena los vectores\n",
    "        vector = torch.cat((d_vec,t_vec), axis=1)\n",
    "        if word_frec[t] > 1:\n",
    "            for j in range(word_frec[t]):\n",
    "                #Obtiene el data set x\n",
    "                x.append(vector)\n",
    "                #Obtiene la supervisión si el término \n",
    "                #está en el documento\n",
    "                y.append(1)\n",
    "        else:\n",
    "            #Obtiene el data set x\n",
    "            x.append(vector)\n",
    "            #Supervisión si el término no está\n",
    "            #en el documento\n",
    "            y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3802a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0085, 0.0109],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0224]]) tensor([0., 0., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#Dataset en formato torch\n",
    "x = torch.stack(x)\n",
    "y = torch.Tensor(y)\n",
    "x = x.view(-1,n_terms+n_docs)\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a695d",
   "metadata": {},
   "source": [
    "### Creación y entrenamiento de la red\n",
    "\n",
    "Nuestra red será una red FeedForward que contendrá cuántas capas ocultas consideremos necesarios. Para definir su arquitectura, usamos la función de Sequential de torch.\n",
    "\n",
    "La saluda de la red será una sola neurona que utiliza función sigmoide, para que así obtengamos una similitud entre 0 y 1. En este caso, podemos interpretar esta similitud como la probabilidad de que el documento contenga al término."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587adb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensiones de las capas ocultas\n",
    "dim = 128\n",
    "#Definimos arquitectura de la red\n",
    "net = nn.Sequential(nn.Linear(n_docs+n_terms, dim), nn.Tanh(), nn.Linear(dim, 2*dim), nn.Tanh(),\n",
    "                    nn.Linear(2*dim, 3*dim), nn.ReLU(), nn.Linear(3*dim, 1), nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33297ccc",
   "metadata": {},
   "source": [
    "Para entrenar la red usamos una función de riesgo para clasificación binaria. Definimos también un análisis por minibatches para que sea más rápido el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bfbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:27<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 32s, sys: 6.28 s, total: 14min 39s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Función de riesgo\n",
    "risk = nn.BCELoss()\n",
    "#Optimizador\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "#Tamaño de mini batches\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    #Permutación de los datos para minibatches\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "    \n",
    "    for i in range(0,x.size()[0], batch_size):\n",
    "        #Obtención de los minibatches\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x[indices], y[indices]\n",
    "        #Predición\n",
    "        pred = net(batch_x)\n",
    "        #Valores reales\n",
    "        y_real = batch_y.reshape(pred.shape)\n",
    "        #Cálculo del riesgo\n",
    "        loss = risk(pred, y_real)\n",
    "        #Paso backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98f035",
   "metadata": {},
   "source": [
    "### Aplicación de la red para predicción de similitud\n",
    "\n",
    "Finalmente, podemos obtener valores de similitud a partir de la red. Dada un término y un documento, la red nos regresará una probabilidad sigmoide que representará la similitud entre ambos (en base al modelo con el que hemos entrenado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841974de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(term, doc):\n",
    "    #Genera vector de entrada a partir del término y el documento\n",
    "    t_vec, doc_vec = X.T[terms[term]], X[docs[doc]]\n",
    "    x_input = torch.cat( (torch.Tensor(doc_vec.todense()), torch.Tensor(t_vec.todense())), axis=1 )\n",
    "    \n",
    "    #Aplica y regresa valores de la red\n",
    "    return net(x_input)\n",
    "\n",
    "def consult(term):\n",
    "    for doc in docs.keys():\n",
    "        s = forward(term, doc)\n",
    "        \n",
    "        yield doc, brown.categories(doc)[0], s.detach().numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bf108",
   "metadata": {},
   "source": [
    "Podemos ver cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ea9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cd04', 'religion', 0.9367077)\n",
      "('cp03', 'romance', 0.93440336)\n",
      "('cd03', 'religion', 0.9342154)\n",
      "('cp04', 'romance', 0.9336889)\n",
      "('cd02', 'religion', 0.9312527)\n",
      "('cd01', 'religion', 0.9304013)\n",
      "('cp05', 'romance', 0.9300379)\n",
      "('cp02', 'romance', 0.92487204)\n",
      "('cp01', 'romance', 0.9155621)\n",
      "('cd05', 'religion', 0.9044896)\n"
     ]
    }
   ],
   "source": [
    "#Resultados de query\n",
    "result = consult('spirit')\n",
    "\n",
    "#Imprimir en orden\n",
    "for r in sorted(result, key=itemgetter(2), reverse=True):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543b9fb",
   "metadata": {},
   "source": [
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
